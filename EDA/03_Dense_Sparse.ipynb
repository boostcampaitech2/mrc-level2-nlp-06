{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(2021)\n",
    "np.random.seed(2021)\n",
    "random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = \"../../data/\"\n",
    "context_path = \"wikipedia_documents.json\"\n",
    "with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "\n",
    "corpus = list(dict.fromkeys([v[\"text\"] for v in wiki.values()]))\n",
    "print('context len :', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "dataset_dir = '../../data/train_dataset'\n",
    "dataset = load_from_disk(dataset_dir)\n",
    "training_dataset = concatenate_datasets([\n",
    "        dataset[\"train\"].flatten_indices(),\n",
    "        dataset[\"validation\"].flatten_indices(),\n",
    "    ])\n",
    "print(len(dataset['train']), len(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sparse Embedding Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv = TfidfVectorizer(tokenizer=tokenizer.tokenize, ngram_range=(1, 2), max_features=50000)\n",
    "p_embedding = tfidfv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_similarity(qeury_vec, k):\n",
    "    result = qeury_vec * p_embedding.T\n",
    "    result = result.toarray()\n",
    "\n",
    "    doc_scores3 = np.partition(result, -k)[:, -k:][:, ::-1]\n",
    "    ind = np.argsort(doc_scores3, axis=-1)[:, ::-1]\n",
    "    doc_scores3 = np.sort(doc_scores3, axis=-1)[:, ::-1]\n",
    "    doc_indices3 = np.argpartition(result, -k)[:, -k:][:, ::-1]\n",
    "    r, c = ind.shape\n",
    "    ind = ind + np.tile(np.arange(r).reshape(-1, 1), (1, c)) * c\n",
    "    doc_indices3 = doc_indices3.ravel()[ind].reshape(r, c)\n",
    "\n",
    "    return doc_scores3, doc_indices3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse_answer_rank_list = []\n",
    "for idx in tqdm(range(len(dataset['validation']))):\n",
    "    query = dataset['validation'][idx]['question']\n",
    "    ground_truth = dataset['validation'][idx]['context']\n",
    "    #print(ground_truth)\n",
    "    query_vec = tfidfv.transform([query])\n",
    "    k=len(corpus)\n",
    "\n",
    "    doc_scores, doc_indices = get_topk_similarity(query_vec, k)\n",
    "\n",
    "sparse_answer_rank_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dense Embedding Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resverse_topk_similarity(qeury_vec, k):\n",
    "    result = qeury_vec * p_embedding.T\n",
    "    result = result.toarray()\n",
    "\n",
    "    doc_scores3 = np.partition(result, k)[:, :k][:, ::-1]\n",
    "    ind = np.argsort(doc_scores3, axis=-1)[:, :]# ::-1]\n",
    "    doc_scores3 = np.sort(doc_scores3, axis=-1)[:, :]# ::-1]\n",
    "    doc_indices3 = np.argpartition(result, k)[:, :k][:, :]# ::-1]\n",
    "    r, c = ind.shape\n",
    "    ind = ind + np.tile(np.arange(r).reshape(-1, 1), (1, c)) * c\n",
    "    doc_indices3 = doc_indices3.ravel()[ind].reshape(r, c)\n",
    "\n",
    "    return doc_scores3, doc_indices3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = tfidfv.transform([training_dataset['context'][1]])\n",
    "result = query_vec * p_embedding.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_resverse_topk_similarity(query_vec, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[25734]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of neagative sample\n",
    "num_neg = 3\n",
    "corpus = np.array(corpus)\n",
    "\n",
    "query_vec = tfidfv.transform(training_dataset['context'])\n",
    "doc_scores, doc_indices = get_resverse_topk_similarity(query_vec, 3)\n",
    "neg_idxs = doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_with_neg = []\n",
    "for idx, c in enumerate(training_dataset['context']):\n",
    "    p_neg = corpus[neg_idxs[idx]]\n",
    "    #print(p_neg)#\n",
    "    #if idx==2: break\n",
    "    p_with_neg.append(c)\n",
    "    p_with_neg.extend(p_neg)\n",
    "#print(p_with_neg)\n",
    "\n",
    "print('[Positive context]')\n",
    "print(p_with_neg[4], '\\n')\n",
    "print('[Negative context]')\n",
    "print(p_with_neg[5], '\\n', p_with_neg[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n",
    "\n",
    "q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(p_with_neg, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "max_len = p_seqs['input_ids'].size(-1)\n",
    "p_seqs['input_ids'] = p_seqs['input_ids'].view(-1, num_neg+1, max_len)\n",
    "p_seqs['attention_mask'] = p_seqs['attention_mask'].view(-1, num_neg+1, max_len)\n",
    "p_seqs['token_type_ids'] = p_seqs['token_type_ids'].view(-1, num_neg+1, max_len)\n",
    "\n",
    "print(p_seqs['input_ids'].size())  #(num_example, pos + neg, max_len)\n",
    "\n",
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BertEncoder, self).__init__(config)\n",
    "\n",
    "    self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "      \n",
    "  def forward(self, input_ids, \n",
    "              attention_mask=None, token_type_ids=None): \n",
    "  \n",
    "      outputs = self.bert(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids)\n",
    "      \n",
    "      pooled_output = outputs[1]\n",
    "      return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model on cuda (if available)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).cuda()\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, num_neg, dataset, p_model, q_model):\n",
    "  \n",
    "  # Dataloader\n",
    "  train_sampler = RandomSampler(dataset)\n",
    "  train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
    "\n",
    "  # Optimizer\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in p_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in p_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in q_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in q_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "  # Start training!\n",
    "  global_step = 0\n",
    "  \n",
    "  p_model.zero_grad()\n",
    "  q_model.zero_grad()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "  for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "      q_encoder.train()\n",
    "      p_encoder.train()\n",
    "      \n",
    "      targets = torch.zeros(args.per_device_train_batch_size).long()\n",
    "      if torch.cuda.is_available():\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        targets = targets.cuda()\n",
    "\n",
    "      p_inputs = {'input_ids': batch[0].view(\n",
    "                                    args.per_device_train_batch_size*(num_neg+1), -1),\n",
    "                  'attention_mask': batch[1].view(\n",
    "                                    args.per_device_train_batch_size*(num_neg+1), -1),\n",
    "                  'token_type_ids': batch[2].view(\n",
    "                                    args.per_device_train_batch_size*(num_neg+1), -1)\n",
    "                  }\n",
    "      \n",
    "      q_inputs = {'input_ids': batch[3],\n",
    "                  'attention_mask': batch[4],\n",
    "                  'token_type_ids': batch[5]}\n",
    "      \n",
    "      p_outputs = p_model(**p_inputs)  #(batch_size*(num_neg+1), emb_dim)\n",
    "      q_outputs = q_model(**q_inputs)  #(batch_size*, emb_dim)\n",
    "\n",
    "      # Calculate similarity score & loss\n",
    "      p_outputs = p_outputs.view(args.per_device_train_batch_size, -1, num_neg+1)\n",
    "      q_outputs = q_outputs.view(args.per_device_train_batch_size, 1, -1)\n",
    "\n",
    "      sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()  #(batch_size, num_neg+1)\n",
    "      sim_scores = sim_scores.view(args.per_device_train_batch_size, -1)\n",
    "      sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "      loss = F.nll_loss(sim_scores, targets)\n",
    "      #print(loss)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      q_model.zero_grad()\n",
    "      p_model.zero_grad()\n",
    "      global_step += 1\n",
    "      \n",
    "      torch.cuda.empty_cache()\n",
    "  return p_model, q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    save_epochs=1,\n",
    ")\n",
    "p_encoder, q_encoder = train(args, num_neg, train_dataset, p_encoder, q_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_corpus = list(set([example['context'] for example in dataset['validation']]))\n",
    "valid_corpus_idx = [np.where(corpus==e)[0].tolist()[0] for e in valid_corpus] # valid문서들의 전체문서 idx\n",
    "valid_corpus_idx = np.array(valid_corpus_idx)\n",
    "\n",
    "answer_dense_rank_list = []\n",
    "for idx in tqdm(range(len(dataset['validation']))):\n",
    "    # 1. query와 정답을 뽑아내기\n",
    "    query = dataset['validation'][idx]['question']\n",
    "    ground_truth = dataset['validation'][idx]['context']\n",
    "\n",
    "    if not ground_truth in valid_corpus:\n",
    "        valid_corpus.append(ground_truth) \n",
    "    # print(query)\n",
    "    # print(ground_truth, '\\n\\n')\n",
    "\n",
    "    # 2. passage encoder, question encoder을 이용해 dense embedding 생성\n",
    "    with torch.no_grad():\n",
    "        p_encoder.eval()\n",
    "        q_encoder.eval()\n",
    "\n",
    "        q_seqs_val = tokenizer([query], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "        q_emb = q_encoder(**q_seqs_val).to('cpu')  #(num_query, emb_dim)\n",
    "\n",
    "        p_embs = []\n",
    "        for p in valid_corpus:\n",
    "            p = tokenizer(p, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "            p_emb = p_encoder(**p).to('cpu').numpy()\n",
    "            p_embs.append(p_emb)\n",
    "\n",
    "    p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n",
    "    #print(p_embs.size(), q_emb.size())\n",
    "\n",
    "    # 3. 생성된 embedding에 dot product를 수행 => Document들의 similarity ranking을 구함\n",
    "    dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "    #print(dot_prod_scores.size())\n",
    "    rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "    #print(dot_prod_scores)\n",
    "    #print(rank)\n",
    "\n",
    "    # 4. rank에서 groundtruth와 비교\n",
    "    rank_doc_idx = valid_corpus_idx[rank.tolist()] # rank된 문서들의 전체문서 idx\n",
    "    ground_truth_doc_idx = np.where(corpus==ground_truth)[0].tolist()[0] # ground truth의 전체문서 idx\n",
    "    answer_rank = np.where(rank_doc_idx==ground_truth_doc_idx)[0].tolist()[0] # 정답문서의 rank\n",
    "    print(answer_rank)\n",
    "    answer_dense_rank_list.append(answer_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
